{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import scipy.sparse as spp\n",
    "import torch\n",
    "from spotlight.evaluation import rmse_score\n",
    "from collections import defaultdict\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process items to match Interactions object in Spotlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users = []\n",
    "items = []\n",
    "score = []\n",
    "timestamp = []\n",
    "helpful = []\n",
    "    \n",
    "with open('reviews.training.json', 'r') as f:\n",
    "    for line in f:\n",
    "        x = json.loads(line)\n",
    "        users.append(x['reviewerID'])\n",
    "        items.append(x['asin'])\n",
    "        score.append(x['overall'])\n",
    "        timestamp.append(x['unixReviewTime'])\n",
    "        helpful.append(x['helpful'])\n",
    "\n",
    "trainusers = users[:]\n",
    "trainitems = items[:]\n",
    "        \n",
    "with open('reviews.dev.json', 'r') as g:\n",
    "    for line in g:\n",
    "        x = json.loads(line)\n",
    "        users.append(x['reviewerID'])\n",
    "        items.append(x['asin'])\n",
    "\n",
    "with open('reviews.test.unlabeled.csv', 'r') as h:\n",
    "    next(h, None)\n",
    "    for line in h:\n",
    "        x = line.split(',')\n",
    "        users.append(x[1].strip())\n",
    "        items.append(x[2].strip())\n",
    "\n",
    "user_labels = {y: x for x, y in enumerate(sorted(set(users)))}\n",
    "item_labels = {y: x for x, y in enumerate(sorted(set(items)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_index = np.int32([user_labels[x] for x in trainusers])\n",
    "item_index = np.int32([item_labels[x] for x in trainitems])\n",
    "\n",
    "score = np.float32(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = spp.coo_matrix((score, (user_index, item_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123960, 51744)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(users)), len(set(items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123952, 50050)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(trainusers)), len(set(trainitems))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process dev items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "devusers = []\n",
    "devitems = []\n",
    "devscore = []\n",
    "dev_tups = []\n",
    "\n",
    "with open('reviews.dev.json', 'r') as g:\n",
    "    for line in g:\n",
    "        x = json.loads(line)\n",
    "        devusers.append(x['reviewerID'])\n",
    "        devitems.append(x['asin'])\n",
    "        devscore.append(x['overall'])\n",
    "        dev_tups.append((user_labels[x['reviewerID']], item_labels[x['asin']], x['overall']))\n",
    "\n",
    "dev_ui = np.int32([user_labels[x] for x in devusers])\n",
    "dev_ii = np.int32([item_labels[x] for x in devitems])\n",
    "\n",
    "devscore = np.float32(devscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spotlight Models: Explicit Factorization (full dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spotlight as sl\n",
    "from spotlight.interactions import Interactions\n",
    "import torch\n",
    "from spotlight.factorization.explicit import ExplicitFactorizationModel\n",
    "from spotlight.factorization.implicit import ImplicitFactorizationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sp_train = Interactions(user_ids = user_index, \n",
    "                        item_ids = item_index,\n",
    "                        ratings = score,\n",
    "                        num_users = 123960,\n",
    "                        num_items = 51744 \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sp_dev = Interactions(user_ids = dev_ui, \n",
    "                        item_ids = dev_ii,\n",
    "                        ratings = devscore,\n",
    "                        num_users = 123960,\n",
    "                        num_items = 51744 \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latent = 128\n",
    "iterations = 4\n",
    "mbatch = 1024\n",
    "L2 = 1e-9\n",
    "learning = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expmodel = ExplicitFactorizationModel(loss='regression',\n",
    "                                       embedding_dim=latent,  # latent dimensionality\n",
    "                                       n_iter=iterations,  # number of epochs of training\n",
    "                                       batch_size=mbatch,  # minibatch size\n",
    "                                       l2=L2,  # strength of L2 regularization\n",
    "                                       learning_rate=learning,\n",
    "                                       use_cuda=torch.cuda.is_available()\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss 13.042520043359453\n",
      "Epoch 1: loss 3.242231524969818\n",
      "Epoch 2: loss 1.2984356079320756\n",
      "Epoch 3: loss 0.9573760417591115\n"
     ]
    }
   ],
   "source": [
    "expmodel.fit(sp_train, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8623668, 1.0816336)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse1 = rmse_score(expmodel, sp_train)\n",
    "rmse2 = rmse_score(expmodel, sp_dev)\n",
    "rmse1, rmse2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tups = []\n",
    "\n",
    "with open('reviews.test.unlabeled.csv', 'r') as h:\n",
    "    next(h, None)\n",
    "    for line in h:\n",
    "        x = line.split(',')\n",
    "        test_tups.append((x[0].strip(), user_labels[x[1].strip()], item_labels[x[2].strip()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for x in dev_tups:\n",
    "    predictions = expmodel.predict(np.array([x[0], x[0]], dtype=np.int64), np.array([x[1], x[1]], dtype=np.int64))[0]\n",
    "    if predictions > 5:\n",
    "        predictions = 5\n",
    "    if predictions < 1:\n",
    "        predictions = 1\n",
    "    y_true.append(x[2])\n",
    "    y_pred.append(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0708745074851083"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "true_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expmodel2 = ExplicitFactorizationModel(loss='regression',\n",
    "                                       embedding_dim=128,  # latent dimensionality\n",
    "                                       n_iter=8,  # number of epochs of training\n",
    "                                       batch_size=mbatch,  # minibatch size\n",
    "                                       l2=L2,  # strength of L2 regularization\n",
    "                                       learning_rate=learning,\n",
    "                                       use_cuda=torch.cuda.is_available()\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss 13.089207182095835\n",
      "Epoch 1: loss 3.278919423174481\n",
      "Epoch 2: loss 1.3041685572583241\n",
      "Epoch 3: loss 0.949660100609895\n",
      "Epoch 4: loss 0.8103529811354625\n",
      "Epoch 5: loss 0.6844400304012399\n",
      "Epoch 6: loss 0.5470632019578378\n",
      "Epoch 7: loss 0.40772226946912676\n"
     ]
    }
   ],
   "source": [
    "expmodel2.fit(sp_train, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69616, 27706, 5.0) 4.9988813\n",
      "(75326, 9399, 2.0) 3.4523451\n",
      "(33338, 44134, 5.0) 4.611212\n",
      "(98604, 12860, 3.0) 1.8130231\n",
      "(85381, 15444, 3.0) 4.698665\n",
      "(24044, 23391, 5.0) 4.2680655\n",
      "(55350, 9671, 5.0) 3.9705033\n",
      "(120327, 45513, 5.0) 3.96565\n",
      "(1611, 45869, 5.0) 5\n",
      "(67328, 43732, 5.0) 4.7994933\n",
      "(60921, 21823, 4.0) 4.433361\n",
      "(69654, 49595, 5.0) 3.478121\n",
      "(57987, 41782, 3.0) 3.8890612\n",
      "(59435, 8582, 1.0) 4.7688513\n",
      "(59451, 9625, 5.0) 5\n",
      "(88766, 30047, 4.0) 3.3429453\n",
      "(11995, 15195, 4.0) 4.1063943\n",
      "(76608, 16672, 2.0) 2.2537487\n",
      "(11138, 17614, 5.0) 5\n",
      "(113174, 44869, 5.0) 4.1313214\n",
      "(68519, 4941, 5.0) 4.7420883\n",
      "(117221, 28960, 3.0) 3.1606114\n",
      "(121393, 13106, 3.0) 3.205855\n",
      "(35633, 33759, 4.0) 3.8210588\n",
      "(120045, 49512, 5.0) 5\n",
      "(67037, 5020, 4.0) 3.4852266\n",
      "(43999, 40664, 5.0) 5\n",
      "(85683, 39286, 5.0) 3.6200361\n",
      "(40336, 9764, 3.0) 3.8191195\n",
      "(9938, 9669, 5.0) 4.243241\n",
      "(40170, 191, 5.0) 4.7740946\n",
      "(81278, 14262, 5.0) 5\n",
      "(61990, 933, 3.0) 3.5157237\n",
      "(78207, 23318, 4.0) 3.7575312\n",
      "(36150, 1991, 5.0) 4.42806\n",
      "(83329, 21306, 5.0) 4.916329\n",
      "(13775, 34609, 5.0) 3.5998478\n",
      "(82090, 23927, 2.0) 3.2055871\n",
      "(98065, 15512, 3.0) 5\n",
      "(69095, 8863, 5.0) 4.2093506\n",
      "(110365, 5836, 4.0) 3.5684965\n",
      "(25559, 48067, 5.0) 5\n",
      "(92384, 16769, 5.0) 4.93482\n",
      "(15129, 19286, 4.0) 3.9506168\n",
      "(37298, 22744, 1.0) 4.032318\n",
      "(90157, 6510, 5.0) 4.007214\n",
      "(39323, 1526, 5.0) 5\n",
      "(99731, 1861, 5.0) 5\n",
      "(43879, 46890, 5.0) 4.6084857\n",
      "(101081, 26306, 5.0) 4.0781345\n"
     ]
    }
   ],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for x in dev_tups[:50]:\n",
    "    predictions = expmodel2.predict(np.array([x[0], x[0]], dtype=np.int64), np.array([x[1], x[1]], dtype=np.int64))[0]\n",
    "    if predictions > 5:\n",
    "        predictions = 5\n",
    "    if predictions < 1:\n",
    "        predictions = 1\n",
    "    y_true.append(x[2])\n",
    "    y_pred.append(predictions)\n",
    "    print(x, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0171161321256215"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "true_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for x in dev_tups:\n",
    "    predictions = expmodel2.predict(np.array([x[0], x[0]], dtype=np.int64), np.array([x[1], x[1]], dtype=np.int64))[0]\n",
    "    if predictions > 5:\n",
    "        predictions = 5\n",
    "    if predictions < 1:\n",
    "        predictions = 1\n",
    "    y_true.append(x[2])\n",
    "    y_pred.append(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('submit_sp_moretraining.csv', 'w', newline='') as g:\n",
    "    csvw = csv.writer(g, delimiter=',')\n",
    "    csvw.writerow(['datapointID','overall'])\n",
    "    for x in test_tups:\n",
    "        predictions = expmodel2.predict(np.array([x[1], x[1]], dtype=np.int64), np.array([x[2], x[2]], dtype=np.int64))[0]\n",
    "        if predictions > 5:\n",
    "            predictions = 5\n",
    "        if predictions < 1:\n",
    "            predictions = 1\n",
    "        csvw.writerow([x[0], predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expmodel3 = ExplicitFactorizationModel(loss='regression',\n",
    "                                       embedding_dim=100,  # latent dimensionality\n",
    "                                       n_iter=6,  # number of epochs of training\n",
    "                                       batch_size=mbatch,  # minibatch size\n",
    "                                       l2=1e-7,  # strength of L2 regularization\n",
    "                                       learning_rate=learning,\n",
    "                                       use_cuda=torch.cuda.is_available()\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss 13.603769565760505\n",
      "Epoch 1: loss 3.8392041759914983\n",
      "Epoch 2: loss 1.4731016009035376\n",
      "Epoch 3: loss 1.0013021011952628\n",
      "Epoch 4: loss 0.8485163029370312\n",
      "Epoch 5: loss 0.7418312040054448\n"
     ]
    }
   ],
   "source": [
    "expmodel3.fit(sp_train, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('submit_sp_reducedim.csv', 'w', newline='') as g:\n",
    "    csvw = csv.writer(g, delimiter=',')\n",
    "    csvw.writerow(['datapointID','overall'])\n",
    "    for x in test_tups:\n",
    "        predictions = expmodel3.predict(np.array([x[1], x[1]], dtype=np.int64), np.array([x[2], x[2]], dtype=np.int64))[0]\n",
    "        if predictions > 5:\n",
    "            predictions = 5\n",
    "        if predictions < 1:\n",
    "            predictions = 1\n",
    "        csvw.writerow([x[0], predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9816580881823934"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for x in dev_tups[:50]:\n",
    "    predictions = expmodel3.predict(np.array([x[0], x[0]], dtype=np.int64), np.array([x[1], x[1]], dtype=np.int64))[0]\n",
    "    if predictions > 5:\n",
    "        predictions = 5\n",
    "    if predictions < 1:\n",
    "        predictions = 1\n",
    "    y_true.append(x[2])\n",
    "    y_pred.append(predictions)\n",
    "\n",
    "true_rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "true_rmse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
